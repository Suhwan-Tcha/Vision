# DL Project

# **Image Classification via EfficientNet**

### â–  Factors Considered in Model Development

- Model Variations
- Data Augmentation Techniques
- Batch Size Optimization
- Optimizer Selection
- LR Scheduling
- Regularization
- Fine Tuning
- Number of Epochs

### 1. Model Variations

Considering the computational cost and memory

EfficientNet B0 ~ B3 â†’ B2 or B3

### 2. Data Augmentation Techniques

- test1 [RandomRotation, RandomResizedCrop, RandomAffine, ColorJitter]

```
  transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),

    #data augmentaton ì ìš©
    transforms.RandomRotation(30),# 30ë„ ì´ë‚´ì˜ ëœë¤ íšŒì „
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # 80%~100% í¬ê¸°ì—ì„œ ëœë¤ ìë¦„
    transforms.RandomAffine(degrees=0, shear=20),  # ëœë¤ ê¸°ìš¸ì´ê¸°
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), # ìƒ‰ìƒ ë³€í™”
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
```

![image.png](image.png)

![image.png](56b4a63d-e826-40f3-abe9-87c9ade15c4f.png)

- test2 [RandomRotation(lower rotation slope), RandomResizedCrop, ColorJitter(lower range)]

```
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),

    #data augmentaton ì ìš©
    transforms.RandomRotation(10),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # 80%~100% í¬ê¸°ì—ì„œ ëœë¤ ìë¦„
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # ìƒ‰ìƒ ë³€í™”
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
```

![image.png](image%201.png)

![image.png](image%202.png)

- test3 [Cutmix Image]

![image.png](image%203.png)

- test4 [Cutmix Image(Reduced proportion of mixed images)]

![image.png](image%204.png)

- test5 [Original Image + Cutmix Image]

![image.png](image%205.png)

- test6 [MixUP]

![image.png](image%206.png)

Test 2ì—ì„œ ê°€ì¥ ì¢‹ì€ í•™ìŠµë¥ ì„ ë³´ì„

CutMix ì´ë¯¸ì§€ ì‚¬ìš© ì‹œ í•™ìŠµë¥ ì´ ë§¤ìš° ì €ì¡°í–ˆëŠ”ë°, ì´ëŠ” Oxford-IIIT Pet Datasetì´ ì£¼ë¡œ ê³ ì–‘ì´ì™€ ê°œì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´, CutMixê°€ í´ë˜ìŠ¤ ê°„ ê²½ê³„ë¥¼ ëª¨í˜¸í•˜ê²Œ ë§Œë“¤ì–´ ëª¨ë¸ í•™ìŠµì„ ì €í•´í•œ ê²ƒìœ¼ë¡œ íŒë‹¨ë¨

### 3. Batch Size Optimization

32 â†’ 64 â†’ 128

64ë¡œ ê²°ì •

### 4. Optimizer Selection

- test1

```
optimizer = optim.Adam(model._fc.parameters(), lr=0.001)
```

- test2

```
optimizer = optim.AdamW(model._fc.parameters(), lr=0.001, weight_decay = 0.05)
```

- test3

```
optimizer = optim.AdamW(model._fc.parameters(), lr=0.001, weight_decay = 0.01)
```

- test4

```
optimizer = optim.RMSprop(model._fc.parameters(), lr=0.0005, weight_decay=0.01, momentum=0.9, alpha=0.95) 
```

- test5

```
optimizer = optim.SGD(model._fc.parameters(), lr=0.001, momentum=0.9, weight_decay=0.05)
```

RMSpropìœ¼ë¡œ ê²°ì •

### 5. LR Scheduling

- test1

```jsx
scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
```

- test2

```jsx
scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)
```

- test3

```jsx
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
```

- test4

```jsx
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
```

StepLRë¡œ ê²°ì •

### 6. Regularization

- Adding Regularization to the last layer of EfficientNet

```
model._fc = nn.Sequential(
    nn.Dropout(p=0.2),  
    nn.Linear(model._fc.in_features, num_classes)
)
```

```
model._fc = nn.Sequential(
    nn.Dropout(p=0.3),  
    nn.Linear(model._fc.in_features, num_classes)
)
```

```
model._fc = nn.Sequential(
    nn.Dropout(p=0.5),  # Dropout ì¶”ê°€
    nn.Linear(model._fc.in_features, num_classes)
)
```

- Setting weight_decay in optimizer

```jsx
optimizer = optim.RMSprop(model._fc.parameters(), lr=0.0005, weight_decay=0.001, momentum=0.9, alpha=0.95) 
```

```jsx
optimizer = optim.RMSprop(model._fc.parameters(), lr=0.0005, weight_decay=0.01, momentum=0.9, alpha=0.95) 
```

```jsx
optimizer = optim.RMSprop(model._fc.parameters(), lr=0.0005, weight_decay=0.1, momentum=0.9, alpha=0.95) 
```

dropout = 0.3, weight_decay = 0.01ë¡œ ê²°ì •

### 7. Fine Tuning

- test1 (ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ë‹¤ì‹œ í•™ìŠµë˜ë„ë¡ ì„¤ì •)

```jsx
for param in model.parameters():
    param.requires_grad = True
```

- test2 (ëª¨ë“  íŒŒë¼ë¯¸í„° ê³ ì •í•˜ê³  ë§ˆì§€ë§‰ Layerë§Œ í•™ìŠµí•˜ê²Œ ì„¤ì •)

```jsx
for param in model.parameters():
    param.requires_grad = False  # ëª¨ë“  ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •

# ì˜ˆë¥¼ ë“¤ì–´, ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
for param in model._fc.parameters():
    param.requires_grad = True  # ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë§Œ í•™ìŠµ
```

test1ë¡œ ê²°ì •

### 8. Number of Epochs

5, 10, 20 â†’ 10ìœ¼ë¡œ ê²°ì •

<aside>
ğŸ’¡

**Final Selection Model**

- EfficientNet b2
- Data Augmentation : RandomRotation + RandomResizedCrop + ColorJitter
- Batch Size : 64
- optimizer : RMSprop(model._fc.parameters(), lr=0.0005, weight_decay=0.01, momentum=0.9, alpha=0.95)
- Schedular : StepLR(optimizer, step_size=7, gamma=0.1)
- Regularization : dropout = 0.3, weight_decay = 0.01
- Fine-tuned with all parameters being learned
- epoch= 10

![image.png](image%207.png)

</aside>